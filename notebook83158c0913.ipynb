{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98ac5bf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:15.084691Z",
     "iopub.status.busy": "2024-11-13T09:09:15.084240Z",
     "iopub.status.idle": "2024-11-13T09:09:16.007393Z",
     "shell.execute_reply": "2024-11-13T09:09:16.006247Z"
    },
    "papermill": {
     "duration": 0.938674,
     "end_time": "2024-11-13T09:09:16.010098",
     "exception": false,
     "start_time": "2024-11-13T09:09:15.071424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9b1621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:16.033013Z",
     "iopub.status.busy": "2024-11-13T09:09:16.032387Z",
     "iopub.status.idle": "2024-11-13T09:09:47.466456Z",
     "shell.execute_reply": "2024-11-13T09:09:47.465091Z"
    },
    "papermill": {
     "duration": 31.457744,
     "end_time": "2024-11-13T09:09:47.478336",
     "exception": false,
     "start_time": "2024-11-13T09:09:16.020592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-generativeai>=0.8.3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcf6042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:47.501468Z",
     "iopub.status.busy": "2024-11-13T09:09:47.500959Z",
     "iopub.status.idle": "2024-11-13T09:09:48.729370Z",
     "shell.execute_reply": "2024-11-13T09:09:48.728218Z"
    },
    "papermill": {
     "duration": 1.243733,
     "end_time": "2024-11-13T09:09:48.732339",
     "exception": false,
     "start_time": "2024-11-13T09:09:47.488606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f858fbdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:48.756426Z",
     "iopub.status.busy": "2024-11-13T09:09:48.755758Z",
     "iopub.status.idle": "2024-11-13T09:09:48.892647Z",
     "shell.execute_reply": "2024-11-13T09:09:48.891564Z"
    },
    "papermill": {
     "duration": 0.152524,
     "end_time": "2024-11-13T09:09:48.895415",
     "exception": false,
     "start_time": "2024-11-13T09:09:48.742891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e244b6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:48.918260Z",
     "iopub.status.busy": "2024-11-13T09:09:48.917653Z",
     "iopub.status.idle": "2024-11-13T09:09:50.420531Z",
     "shell.execute_reply": "2024-11-13T09:09:50.419198Z"
    },
    "papermill": {
     "duration": 1.51813,
     "end_time": "2024-11-13T09:09:50.424007",
     "exception": false,
     "start_time": "2024-11-13T09:09:48.905877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a really smart robot friend. It can learn things just like you do, but much faster!  \n",
      "\n",
      "This robot friend is called **Artificial Intelligence** or **AI** for short. It uses something called **machine learning** to learn from lots of data, like reading millions of books or watching thousands of videos. \n",
      "\n",
      "Here's what AI can do:\n",
      "\n",
      "* **Play games:** It can beat you at chess or even learn to play your favorite video game!\n",
      "* **Answer your questions:**  AI can help you find information online or even write a story for you!\n",
      "* **Help doctors:** AI can help doctors diagnose illnesses and create better treatments.\n",
      "* **Drive cars:** AI can help cars drive themselves!\n",
      "* **Make art:** AI can even paint pictures and write music!\n",
      "\n",
      "AI is still learning and growing, but it's already making a big difference in the world. It can help us solve problems and make our lives easier. Isn't that cool? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6fcc833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:50.457388Z",
     "iopub.status.busy": "2024-11-13T09:09:50.456378Z",
     "iopub.status.idle": "2024-11-13T09:09:50.474224Z",
     "shell.execute_reply": "2024-11-13T09:09:50.467769Z"
    },
    "papermill": {
     "duration": 0.036345,
     "end_time": "2024-11-13T09:09:50.478137",
     "exception": false,
     "start_time": "2024-11-13T09:09:50.441792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really smart robot friend. It can learn things just like you do, but much faster!  \n",
       "\n",
       "This robot friend is called **Artificial Intelligence** or **AI** for short. It uses something called **machine learning** to learn from lots of data, like reading millions of books or watching thousands of videos. \n",
       "\n",
       "Here's what AI can do:\n",
       "\n",
       "* **Play games:** It can beat you at chess or even learn to play your favorite video game!\n",
       "* **Answer your questions:**  AI can help you find information online or even write a story for you!\n",
       "* **Help doctors:** AI can help doctors diagnose illnesses and create better treatments.\n",
       "* **Drive cars:** AI can help cars drive themselves!\n",
       "* **Make art:** AI can even paint pictures and write music!\n",
       "\n",
       "AI is still learning and growing, but it's already making a big difference in the world. It can help us solve problems and make our lives easier. Isn't that cool? \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa95143c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:50.512917Z",
     "iopub.status.busy": "2024-11-13T09:09:50.512359Z",
     "iopub.status.idle": "2024-11-13T09:09:50.896250Z",
     "shell.execute_reply": "2024-11-13T09:09:50.894989Z"
    },
    "papermill": {
     "duration": 0.405016,
     "end_time": "2024-11-13T09:09:50.898925",
     "exception": false,
     "start_time": "2024-11-13T09:09:50.493909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Zlork! It's nice to meet you. ðŸ˜Š What can I do for you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = flash.start_chat(history=[])\n",
    "response = chat.send_message('Hello! My name is Zlork.')\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5818880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:50.923703Z",
     "iopub.status.busy": "2024-11-13T09:09:50.923288Z",
     "iopub.status.idle": "2024-11-13T09:09:51.843222Z",
     "shell.execute_reply": "2024-11-13T09:09:51.841979Z"
    },
    "papermill": {
     "duration": 0.936167,
     "end_time": "2024-11-13T09:09:51.846229",
     "exception": false,
     "start_time": "2024-11-13T09:09:50.910062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Dinosaurs are fascinating creatures. Here's a fun fact: \n",
      "\n",
      "**Did you know that some dinosaurs had feathers?** \n",
      "\n",
      "While we often picture dinosaurs as scaly behemoths, paleontologists have discovered many dinosaur fossils with well-preserved feather impressions. This evidence suggests that feathers were not exclusive to birds, but likely evolved much earlier in the dinosaur lineage. Some dinosaurs, like the Velociraptor, may have even sported vibrant colors and patterns on their feathers! \n",
      "\n",
      "What else would you like to know about dinosaurs?  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell something interesting about dinosaurs?')\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5d8637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:51.871868Z",
     "iopub.status.busy": "2024-11-13T09:09:51.871377Z",
     "iopub.status.idle": "2024-11-13T09:09:52.503332Z",
     "shell.execute_reply": "2024-11-13T09:09:52.502070Z"
    },
    "papermill": {
     "duration": 0.647297,
     "end_time": "2024-11-13T09:09:52.506015",
     "exception": false,
     "start_time": "2024-11-13T09:09:51.858718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I do! You said your name is Zlork. ðŸ˜Š \n",
      "\n",
      "I'm still getting to know you, but I'm trying my best to remember.  It's nice to meet you, Zlork! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# While you have the `chat` object around, the conversation state\n",
    "# persists. Confirm that by asking if it knows my name.\n",
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c8a019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:52.531368Z",
     "iopub.status.busy": "2024-11-13T09:09:52.530901Z",
     "iopub.status.idle": "2024-11-13T09:09:52.725592Z",
     "shell.execute_reply": "2024-11-13T09:09:52.723968Z"
    },
    "papermill": {
     "duration": 0.210299,
     "end_time": "2024-11-13T09:09:52.728084",
     "exception": false,
     "start_time": "2024-11-13T09:09:52.517785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  print(model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9afda617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:52.752548Z",
     "iopub.status.busy": "2024-11-13T09:09:52.752103Z",
     "iopub.status.idle": "2024-11-13T09:09:52.943459Z",
     "shell.execute_reply": "2024-11-13T09:09:52.942059Z"
    },
    "papermill": {
     "duration": 0.206942,
     "end_time": "2024-11-13T09:09:52.946373",
     "exception": false,
     "start_time": "2024-11-13T09:09:52.739431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  if model.name == 'models/gemini-1.5-flash':\n",
    "    print(model)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eca228e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:52.971671Z",
     "iopub.status.busy": "2024-11-13T09:09:52.971241Z",
     "iopub.status.idle": "2024-11-13T09:09:54.342284Z",
     "shell.execute_reply": "2024-11-13T09:09:54.340993Z"
    },
    "papermill": {
     "duration": 1.386861,
     "end_time": "2024-11-13T09:09:54.345031",
     "exception": false,
     "start_time": "2024-11-13T09:09:52.958170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Enduring Significance of the Olive: From Ancient Times to Modern Society\n",
      "\n",
      "The olive, a small, unassuming fruit, has played a pivotal role in human civilization for millennia. From its humble origins in the Mediterranean basin, it has traversed continents and cultures, becoming an integral part of both culinary traditions and economic landscapes. Today, in the intricate tapestry of modern society, the olive continues to weave its own unique thread, impacting our health, economy, and cultural identity.\n",
      "\n",
      "The olive's importance is deeply rooted in its versatility. Its fleshy fruit yields oil, a culinary staple revered for its flavor, health benefits, and historical significance. Olive oil has been used for cooking, lighting, and even medicine for centuries, playing a crucial role in sustaining ancient civilizations.  The olive tree itself is a source of timber, its wood prized for its durability and beauty. Its leaves hold medicinal properties, while its branches provide shade and fuel. This multifaceted nature made the olive a symbol of abundance and life\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
    "\n",
    "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f8ad38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:54.370574Z",
     "iopub.status.busy": "2024-11-13T09:09:54.369859Z",
     "iopub.status.idle": "2024-11-13T09:09:55.307974Z",
     "shell.execute_reply": "2024-11-13T09:09:55.306679Z"
    },
    "papermill": {
     "duration": 0.953665,
     "end_time": "2024-11-13T09:09:55.310574",
     "exception": false,
     "start_time": "2024-11-13T09:09:54.356909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ancient groves to modern plates,\n",
      "The olive reigns, its flavor mates\n",
      "With savory bites, a salty kiss,\n",
      "A verdant jewel, a culinary bliss.\n",
      "\n",
      "From fragrant oils to vibrant tapenade,\n",
      "Its taste and health, a gift well-made.\n",
      "A symbol of peace, a symbol of grace,\n",
      "The olive's presence, a cherished embrace. \n",
      "\n",
      "In kitchens grand and simple meals,\n",
      "A culinary hero, its worth reveals.\n",
      "From salad bowls to pasta strands,\n",
      "The olive's bounty, in countless hands. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e8cc0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:55.336108Z",
     "iopub.status.busy": "2024-11-13T09:09:55.335685Z",
     "iopub.status.idle": "2024-11-13T09:09:57.036494Z",
     "shell.execute_reply": "2024-11-13T09:09:57.035064Z"
    },
    "papermill": {
     "duration": 1.716979,
     "end_time": "2024-11-13T09:09:57.039339",
     "exception": false,
     "start_time": "2024-11-13T09:09:55.322360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      " -------------------------\n",
      "Purple. \n",
      " -------------------------\n",
      "Blue \n",
      " -------------------------\n",
      "Blue \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                              request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3c9a3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:57.066095Z",
     "iopub.status.busy": "2024-11-13T09:09:57.065076Z",
     "iopub.status.idle": "2024-11-13T09:09:58.869440Z",
     "shell.execute_reply": "2024-11-13T09:09:58.868192Z"
    },
    "papermill": {
     "duration": 1.820688,
     "end_time": "2024-11-13T09:09:58.872282",
     "exception": false,
     "start_time": "2024-11-13T09:09:57.051594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(5):\n",
    "  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                             request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdc0a913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:09:58.899698Z",
     "iopub.status.busy": "2024-11-13T09:09:58.899235Z",
     "iopub.status.idle": "2024-11-13T09:10:20.673171Z",
     "shell.execute_reply": "2024-11-13T09:10:20.671777Z"
    },
    "papermill": {
     "duration": 21.790568,
     "end_time": "2024-11-13T09:10:20.675756",
     "exception": false,
     "start_time": "2024-11-13T09:09:58.885188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartholomew, a ginger tabby with a penchant for mischief and a disdain for routine, had a problem. He had awoken one Tuesday morning to find his breakfast bowl empty. Utterly empty. Not a single kibble remained. This was an outrage, an affront to his feline dignity. His human, a kind but forgetful creature named Emily, was nowhere in sight. \n",
      "\n",
      "Bartholomew, ever the resourceful cat, decided to take matters into his own paws. He'd heard whispers in the alleyways, rumors of a mystical place called \"The Fish Market.\" A place overflowing with tuna, salmon, and even those elusive, delicious sardines he'd only dreamt of. \n",
      "\n",
      "He snuck out through the cat flap, his whiskers twitching with anticipation. The world outside was a whirlwind of smells and sounds. He navigated the labyrinthine back alleys, dodging garbage bins and gossiping pigeons. Finally, he saw it - a grand, bustling place filled with the irresistible scent of the sea.\n",
      "\n",
      "The Fish Market was a cacophony of splashing water, chattering humans, and the pungent aroma of fresh fish. Bartholomew, ever the cautious feline, moved through the crowd, his tail twitching nervously. He saw glistening mounds of silvery fish, their scales catching the light. He saw large, fleshy crabs, their claws snapping in the air. \n",
      "\n",
      "Suddenly, a hand reached out, offering him a piece of raw salmon. He looked up to see a kind-faced fishmonger, his eyes twinkling. The fishmonger, a man named Mr. Finnigan, was a cat lover, and he couldn't resist Bartholomew's pleading eyes.\n",
      "\n",
      "Bartholomew, overwhelmed with gratitude, devoured the salmon, a purr rumbling deep in his chest. He knew he had found a new friend, a haven of deliciousness, and a secret he would forever treasure.\n",
      "\n",
      "As the sun set, he padded back home, his belly full, a new confidence in his stride. Emily greeted him with a playful swat and a bowl of kibble, but Bartholomew just sniffed it disdainfully. He had tasted the ocean, and the world, and that was all that mattered. His adventure was just beginning. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b78a07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:20.703211Z",
     "iopub.status.busy": "2024-11-13T09:10:20.702734Z",
     "iopub.status.idle": "2024-11-13T09:10:20.996141Z",
     "shell.execute_reply": "2024-11-13T09:10:20.994586Z"
    },
    "papermill": {
     "duration": 0.310493,
     "end_time": "2024-11-13T09:10:20.998998",
     "exception": false,
     "start_time": "2024-11-13T09:10:20.688505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE**\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5,\n",
    "    ))\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c689ccd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:21.026968Z",
     "iopub.status.busy": "2024-11-13T09:10:21.026535Z",
     "iopub.status.idle": "2024-11-13T09:10:21.796278Z",
     "shell.execute_reply": "2024-11-13T09:10:21.794915Z"
    },
    "papermill": {
     "duration": 0.787078,
     "end_time": "2024-11-13T09:10:21.798985",
     "exception": false,
     "start_time": "2024-11-13T09:10:21.011907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43bfabde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:21.833915Z",
     "iopub.status.busy": "2024-11-13T09:10:21.833076Z",
     "iopub.status.idle": "2024-11-13T09:10:22.245060Z",
     "shell.execute_reply": "2024-11-13T09:10:22.243702Z"
    },
    "papermill": {
     "duration": 0.435421,
     "end_time": "2024-11-13T09:10:22.248386",
     "exception": false,
     "start_time": "2024-11-13T09:10:21.812965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a43cf25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:22.279245Z",
     "iopub.status.busy": "2024-11-13T09:10:22.278700Z",
     "iopub.status.idle": "2024-11-13T09:10:22.708457Z",
     "shell.execute_reply": "2024-11-13T09:10:22.706975Z"
    },
    "papermill": {
     "duration": 0.447641,
     "end_time": "2024-11-13T09:10:22.711436",
     "exception": false,
     "start_time": "2024-11-13T09:10:22.263795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b11481ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:22.739822Z",
     "iopub.status.busy": "2024-11-13T09:10:22.739394Z",
     "iopub.status.idle": "2024-11-13T09:10:23.242446Z",
     "shell.execute_reply": "2024-11-13T09:10:23.241171Z"
    },
    "papermill": {
     "duration": 0.520378,
     "end_time": "2024-11-13T09:10:23.245320",
     "exception": false,
     "start_time": "2024-11-13T09:10:22.724942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you were 4, your partner was 3 * 4 = 12 years old.\n",
      "\n",
      "This means your partner is 12 - 4 = 8 years older than you.\n",
      "\n",
      "Now that you are 20, your partner is 20 + 8 = **28 years old**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afe24e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:23.274025Z",
     "iopub.status.busy": "2024-11-13T09:10:23.273566Z",
     "iopub.status.idle": "2024-11-13T09:10:23.978083Z",
     "shell.execute_reply": "2024-11-13T09:10:23.976704Z"
    },
    "papermill": {
     "duration": 0.722029,
     "end_time": "2024-11-13T09:10:23.980721",
     "exception": false,
     "start_time": "2024-11-13T09:10:23.258692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this:\n",
      "\n",
      "* **When you were 4, your partner was 3 times your age:** 4 years old * 3 = 12 years old\n",
      "* **Age difference:** Your partner was 12 - 4 = 8 years older than you.\n",
      "* **Age difference remains the same:**  Since the age difference stays constant, your partner is still 8 years older than you.\n",
      "* **Your partner's current age:** 20 years old (your age) + 8 years (age difference) = **28 years old** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5509b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:24.010028Z",
     "iopub.status.busy": "2024-11-13T09:10:24.009611Z",
     "iopub.status.idle": "2024-11-13T09:10:24.018630Z",
     "shell.execute_reply": "2024-11-13T09:10:24.017365Z"
    },
    "papermill": {
     "duration": 0.026424,
     "end_time": "2024-11-13T09:10:24.021198",
     "exception": false,
     "start_time": "2024-11-13T09:10:23.994774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbd0c5e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:24.049356Z",
     "iopub.status.busy": "2024-11-13T09:10:24.048913Z",
     "iopub.status.idle": "2024-11-13T09:10:29.344972Z",
     "shell.execute_reply": "2024-11-13T09:10:29.343642Z"
    },
    "papermill": {
     "duration": 5.313334,
     "end_time": "2024-11-13T09:10:29.347800",
     "exception": false,
     "start_time": "2024-11-13T09:10:24.034466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to search for the transformers NLP paper and find the authors. Then I need to find the youngest author.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "    [model_instructions, example1, example2, question],\n",
    "    generation_config=config,\n",
    "    request_options=retry_policy)\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a7f9b3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:29.376620Z",
     "iopub.status.busy": "2024-11-13T09:10:29.376186Z",
     "iopub.status.idle": "2024-11-13T09:10:29.918680Z",
     "shell.execute_reply": "2024-11-13T09:10:29.917359Z"
    },
    "papermill": {
     "duration": 0.560013,
     "end_time": "2024-11-13T09:10:29.921338",
     "exception": false,
     "start_time": "2024-11-13T09:10:29.361325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "The observation lists the authors, but not their ages. I need to search for each author individually to find their age.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani age</search> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "494ffd5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:29.951408Z",
     "iopub.status.busy": "2024-11-13T09:10:29.950931Z",
     "iopub.status.idle": "2024-11-13T09:10:30.312781Z",
     "shell.execute_reply": "2024-11-13T09:10:30.311488Z"
    },
    "papermill": {
     "duration": 0.380368,
     "end_time": "2024-11-13T09:10:30.315459",
     "exception": false,
     "start_time": "2024-11-13T09:10:29.935091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ))\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee123ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:30.344836Z",
     "iopub.status.busy": "2024-11-13T09:10:30.344412Z",
     "iopub.status.idle": "2024-11-13T09:10:31.605821Z",
     "shell.execute_reply": "2024-11-13T09:10:31.604282Z"
    },
    "papermill": {
     "duration": 1.279158,
     "end_time": "2024-11-13T09:10:31.608546",
     "exception": false,
     "start_time": "2024-11-13T09:10:30.329388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I will calculate the sum of the first 14 odd prime numbers. \n",
       "\n",
       "Here are the first 14 odd prime numbers: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43.\n",
       "\n",
       "\n",
       "``` python\n",
       "sum = 2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37 + 41 + 43\n",
       "print(f'sum = {sum}')\n",
       "\n",
       "```\n",
       "```\n",
       "sum = 281\n",
       "\n",
       "```\n",
       "The sum of the first 14 odd prime numbers is 281. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution',)\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1bab5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:31.638053Z",
     "iopub.status.busy": "2024-11-13T09:10:31.637636Z",
     "iopub.status.idle": "2024-11-13T09:10:32.577530Z",
     "shell.execute_reply": "2024-11-13T09:10:32.576036Z"
    },
    "papermill": {
     "duration": 0.95753,
     "end_time": "2024-11-13T09:10:32.580025",
     "exception": false,
     "start_time": "2024-11-13T09:10:31.622495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_primes=326\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "\n",
    "primes = sympy.primerange(0, 100)\n",
    "odd_primes = [p for p in primes if p % 2 != 0]\n",
    "sum_of_primes = sum(odd_primes[:14])\n",
    "print(f'{sum_of_primes=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29ebe09b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:32.611347Z",
     "iopub.status.busy": "2024-11-13T09:10:32.610627Z",
     "iopub.status.idle": "2024-11-13T09:10:32.617917Z",
     "shell.execute_reply": "2024-11-13T09:10:32.616746Z"
    },
    "papermill": {
     "duration": 0.026696,
     "end_time": "2024-11-13T09:10:32.620530",
     "exception": false,
     "start_time": "2024-11-13T09:10:32.593834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"I will calculate the sum of the first 14 odd prime numbers. \\n\\nHere are the first 14 odd prime numbers: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43.\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\nsum = 2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37 + 41 + 43\\nprint(f\\'sum = {sum}\\')\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "  output: \"sum = 281\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"The sum of the first 14 odd prime numbers is 281. \\n\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be365555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T09:10:32.650529Z",
     "iopub.status.busy": "2024-11-13T09:10:32.650066Z",
     "iopub.status.idle": "2024-11-13T09:10:35.486352Z",
     "shell.execute_reply": "2024-11-13T09:10:35.485004Z"
    },
    "papermill": {
     "duration": 2.854127,
     "end_time": "2024-11-13T09:10:35.488985",
     "exception": false,
     "start_time": "2024-11-13T09:10:32.634858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a Bash script that provides a highly customizable Git prompt for your terminal. It's a complex script, but at a high level, here's what it does and why you'd use it:\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "* **Displays Git status information:**  This script dynamically updates your terminal prompt to show your current Git branch, if you're ahead or behind the remote, if you have uncommitted changes, etc.\n",
       "* **Customizable appearance:** It allows you to choose from several pre-defined themes or create your own custom theme to define colors and prompt format.\n",
       "* **Integration with virtual environments:** The script can detect and display your active virtual environment (if any) alongside your Git status.\n",
       "* **Remote status updates:** It fetches remote revisions periodically to keep your prompt up-to-date.\n",
       "\n",
       "**Why use it:**\n",
       "\n",
       "* **Improved Git workflow:** Having your Git status displayed in your terminal prompt can significantly improve your workflow. You can quickly see what branch you're on, whether you need to commit changes, and if your branch is up-to-date.\n",
       "* **Clean and informative prompt:** The script's default and customizable themes allow for a clean and informative prompt that doesn't clutter your terminal.\n",
       "* **Increased efficiency:**  You can quickly identify potential issues and navigate your Git repository with ease.\n",
       "\n",
       "**How to use it:**\n",
       "\n",
       "1. **Download the script:**  Save the code to a file named `git-prompt.sh`.\n",
       "2. **Make it executable:** Run `chmod +x git-prompt.sh`.\n",
       "3. **Source the script:** Add `source ./git-prompt.sh` to your `.bashrc` or `.zshrc` file (depending on your shell).\n",
       "4. **Customize:**  The script provides numerous settings and options to customize your Git prompt.  Refer to the script's comments for details on the various settings and how to modify them.\n",
       "\n",
       "By using this script, you can make your Git workflow more efficient and your terminal prompt more informative.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 84.271887,
   "end_time": "2024-11-13T09:10:36.328423",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-13T09:09:12.056536",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
